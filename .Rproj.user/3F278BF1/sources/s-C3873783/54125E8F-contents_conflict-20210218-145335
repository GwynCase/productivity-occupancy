---
title: "Zeroes"
output: html_notebook
always_allow_html: true
---

```{r options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE)
```

Including or excluding onesies didn't make much of a difference, so I wanted to confirm that it was the zeroes that were driving this weird pattern.

```{r message=FALSE, warning=FALSE}
# Import conflict settings.
source('../src/conflicted.R')

# Load some libraries.
library(tidyverse)
library(ggplot2)
library(raster)
library(sf)
library(landscapemetrics)
library(knitr)
library(kableExtra)
library(broom)
library(AICcmodavg)
library(modelr)

# Load in the processed data from last time.
data <- read_csv('../data/processed/landscape_metrics_zeroes.csv')

# Remove problematic TCR.
df <- data %>% filter(site != 'TCR')

# Keep only sites with occupancy data.
df <- df %>% drop_na(years.surveyed) #90

# Remove sites that are missing sufficient landscape data.
df <- df %>% filter_at(vars(contains('inside')), all_vars(. >=90))
```

How many zeroes are there, and what do they look like?

```{r}
df %>% filter(years.detect == 0) %>% 
  distinct(name, .keep_all=TRUE) %>% 
  select(site, name, years.surveyed, years.detect, years.no.detect)
```

First off, that's 20 (!) sites that are zeroes. Some of these might be resolvable with a more precise data set... I think Red Tusk, for example, had actually been surveyed more than one year. Some of this may also be time-series issue... again, I think Red Tusk has been harvested, so these older occupancy data + more recent VRI data could explain why including zeroes gives me unusual results. But also very interesting that some of these sites have been surveyed for *years* and never been active, like Jarvis Creek at 5 (!) years. That supports the idea that at least some of these are either not NOGO territories or are so low quality that they bear no resemblance to "normal" NOGO territories.

I didn't have ready-calculated HSI diversity, so I can add that (I think...). Start by bringing in the raster and prepping it.

```{r}
# Load HSI raster.
r.hsi <- raster('../data/processed/foraging_sc.tif')

# Define levels.
hsi.levels <- data.frame(ID=c(-10, -2, -1, 0, 1, 2, 3), 
                         class.name=c('ocean', 'freshwater', 'river', 
                                      'nil', 'low', 'moderate', 'high'))

# Add levels to raster.
levels(r.hsi) <- hsi.levels

# Assign crs to raster.
crs(r.hsi) <- CRS('+proj=utm +zone=10 +datum=NAD83 +units=m +no_defs')
```

Then get landscape sizes and sites set up.

```{r message=FALSE}
# Define landscape sizes.
landscape <- data.frame(
  size=c('PFA', 'breeding area', 'home range', 'maximum range'),
  area=c(60, 200, 3800, 15600)
)

# Convert area in hectares to radii in meters.
landscape <- landscape %>% mutate(radius=sqrt(area*10000/pi))

# Read in the data.
nests <- read_csv('../data/processed/sc_nests.csv')

# Calculate a centroid for each site, and keep only ones with a quality index.
centroids <- nests %>% group_by(site) %>% 
  mutate(mean.x=mean(xcoord), mean.y=mean(ycoord)) %>% 
  distinct(site, name, mean.x, mean.y)

sites <- semi_join(centroids, data, by=c('site', 'name')) %>% 
  rename(xcoord=mean.x, ycoord=mean.y)

# Make it a spatial object for later.
sites.sf <- sites %>% 
  st_as_sf(coords=c('xcoord', 'ycoord')) %>% 
  st_set_crs('+proj=utm +zone=10 +datum=WGS84 +units=m +no_defs') %>% 
  st_as_sf()

# Also make a list of site names for later.
site.names <- sites.sf$site
```

Then do the actual calculations.

```{r}
# Make a list of metrics to calculate.
hsi.metrics <- c('lsm_l_sidi')

# Make a function to do the calculations and formatting.
calc.hsi.metrics <- function(x) {
  sample_lsm(r.hsi, y=sites.sf, size=x, plot_id=site.names, shape='circle', 
             what=hsi.metrics) %>% 
    left_join(hsi.levels, by=c('class'='ID')) %>% 
    mutate(class.name=ifelse(is.na(class.name), metric, class.name)) %>% 
    select(-class, -metric, -level) %>%  
    pivot_wider(names_from=class.name, values_from=value) %>% 
    mutate(radius=x) %>% 
    rename(hsi.inside=percentage_inside)
}

# Run the function for each sample size.
hsi.landscape.metrics <- map_df(landscape$radius, calc.hsi.metrics)

# Do some cleanup
hsi.landscape.metrics <- hsi.landscape.metrics %>% 
  select(radius, hsi.inside, nest=plot_id, hsi.diversity=sidi)

hsi.landscape.metrics <- select(landscape, radius, size) %>% right_join(hsi.landscape.metrics, by=c('radius'))

# Join to data frame.
df <- left_join(df, hsi.landscape.metrics, by=c('site'='nest', 'size', 'radius', 'hsi.inside'))

# Filter out any sites without sufficient landscape coverage.
df <- df %>% filter_at(vars(contains('inside')), all_vars(. >=90))
```

# The models

```{r}
# Proportion suitable
proportion.suitable.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.suitable, data=df, family=binomial(logit))
}

# Proportion suitable + HSI diversity
suitable.diversity.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.suitable + hsi.diversity, data=df, family=binomial(logit))
}

# Proportion suitable + suitable edge density
suitable.edge.density.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.suitable + suitable.edge.density, data=df, family=binomial(logit))
}

# Proportion suitable + HSI diversity + suitable edge density
suitable.sink.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.suitable + 
        suitable.edge.density + hsi.diversity, data=df, family=binomial(logit))
}

# Proportion mature forest
proportion.mature.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.cover.mature, data=df, family=binomial(logit))
}

# Proportion mature + landcover diversity
mature.diversity.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.cover.mature + cover.diversity, data=df, family=binomial(logit))
}

# Proportion mature + gap edge density
mature.edge.density.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.cover.mature + gap.edge.density, data=df, family=binomial(logit))
}

# Proportion mature + gap edge density + landcover diversity
mature.sink.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ proportion.cover.mature + 
        gap.edge.density + cover.diversity, data=df, family=binomial(logit))
}

# Null
null.model <- function(df) {
  glm(cbind(years.detect, years.no.detect) ~ 1, data=df, family=binomial(logit))
}
```

Then nest the data frame for ease of coding and apply each function to generate a disturbingly large number of models really fast.

```{r warning=FALSE}
# Nest the data frame.
nf <- df %>% group_by(size) %>% nest()

# Apply the functions.
nf <- nf %>% 
  mutate(
    m.proportion.suitable=map(data, proportion.suitable.model),
    m.suitable.diversity=map(data, suitable.diversity.model),
    m.suitable.edge.density=map(data, suitable.edge.density.model),
    m.suitable.sink=map(data, suitable.sink.model),
    m.proportion.mature=map(data, proportion.mature.model),
    m.mature.diversity=map(data, mature.diversity.model),
    m.mature.edge.density=map(data, mature.edge.density.model),
    m.mature.sink=map(data, mature.sink.model),
    m.null=map(data, null.model)
         )
```

Let's see if there's any difference in the big list of models.

```{r warning=FALSE}
all.models <- nf %>% pivot_longer(-c(size, data), names_to='modname', values_to='model') %>% 
  mutate(name=paste(size, modname))

aictab(all.models$model, modnames=all.models$name) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```

Well, that's reassuring that it's more or less the same. The top model is unchanged. Edge density at the PFA scale is a little less persuasive, but not by much. The nulls are still clustered at the top. The VRI-based models are still doing better than the HSI-based models.

Let's break it down by size.

```{r}
pfa.models <- nf %>% filter(size == 'PFA') %>% 
  pivot_longer(-c(size, data), names_to='modname', values_to='model')

ba.models <- nf %>% filter(size == 'breeding area') %>% 
  pivot_longer(-c(size, data), names_to='modname', values_to='model')

hr.models <- nf %>% filter(size == 'home range') %>% 
  pivot_longer(-c(size, data), names_to='modname', values_to='model')

mr.models <- nf %>% filter(size == 'maximum range') %>% 
  pivot_longer(-c(size, data), names_to='modname', values_to='model')
```


## PFA

```{r}
aictab(pfa.models$model, modnames=pfa.models$modname) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```

So here *all* the models perform better than the null, and HSI-based models generally do better than the VRI-based models.

### Top model

We can visualize the top model. Maybe.

```{r}
# Separate out the data.
pfa.data <- df %>% filter(size == 'PFA')

# Re-write the top model on its own.
pfa.top.model <- glm(cbind(years.detect, years.no.detect) ~ proportion.suitable, data=pfa.data, family=binomial(logit))

# Create a grid and add predictions.
pfa.predicted <- data_grid(pfa.data, proportion.suitable, .model=pfa.top.model) %>% 
  mutate(pred=predict(pfa.top.model, newdata=., type='response'))

# Grab the inverse link function from the model.
inv <- family(pfa.top.model)$linkinv

# Add fit and SE data.
pfa.predicted <- bind_cols(pfa.predicted, setNames(as_tibble(predict(pfa.top.model, pfa.predicted, 
                                                                     se.fit = TRUE)[1:2]),
                           c('fit.link','se.link')))

# Create confidence interval.
pfa.predicted <- pfa.predicted %>% mutate(fit.resp  = inv(fit.link),
                  right.upr = inv(fit.link + (2 * se.link)),
                  right.lwr = inv(fit.link - (2 * se.link)))

# Plot them?
ggplot(pfa.predicted, aes(x=proportion.suitable, y=pred)) +
  geom_line() +
  geom_ribbon(aes(ymin=right.lwr, ymax=right.upr), alpha=0.1) +
  geom_point(data=pfa.data, aes(x=proportion.suitable, y=quality.index)) +
  theme_classic()
```

```{r}
summary(pfa.top.model)
```

```{r}
# Look at some diagnostics.
data.frame(predicted=predict(pfa.top.model, type='response'),
           residuals=residuals(pfa.top.model, type='response')) %>% 
  ggplot(aes(x=predicted, y=residuals)) +
  geom_point() +
  geom_hline(yintercept=0, linetype='dashed') +
  geom_smooth(method='lm', se=FALSE, color='black') +
  theme_classic()
```

Well, the residuals are noticeably worse, so that's unfortunate.

### All models

```{r}
pfa.models %>% mutate(glance=map(model, glance)) %>% 
  unnest(glance) %>% ungroup() %>% 
  select(!c(size, data, model))
```

```{r}
pfa.models %>% mutate(tidy=map(model, tidy)) %>% 
  unnest(tidy) %>% ungroup() %>% 
  select(!c(size, data, model)) %>% 
  filter(term != '(Intercept)')
```

## Breeding area

```{r}
aictab(ba.models$model, modnames=ba.models$modname) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```

Here the VRI-based kitchen sink does *better*, and actually the HSI-based suitable does better than the VRI-based mature--but both still much worse than the null.


### Top model

We can also visualize the top model for the breeding area.

```{r}
# Separate out the data.
ba.data <- df %>% filter(size == 'breeding area')

# Re-write the top model on its own.
ba.top.model <- glm(cbind(years.detect, years.no.detect) ~ proportion.cover.mature + 
                   gap.edge.density, data=ba.data, family=binomial(logit))

# Create a grid and add predictions.
ba.predicted <- data_grid(ba.data, gap.edge.density, .model=ba.top.model) %>% 
  mutate(pred=predict(ba.top.model, newdata=., type='response'))

# Grab the inverse link function from the model.
ba.inv <- family(ba.top.model)$linkinv

# Add fit and SE data.
ba.predicted <- bind_cols(ba.predicted, setNames(as_tibble(predict(ba.top.model, ba.predicted, 
                                                                     se.fit = TRUE)[1:2]),
                           c('fit.link','se.link')))

# Create confidence interval.
ba.predicted <- ba.predicted %>% mutate(fit.resp  = ba.inv(fit.link),
                  right.upr = ba.inv(fit.link + (2 * se.link)),
                  right.lwr = ba.inv(fit.link - (2 * se.link)))

# Plot them?
ggplot(ba.predicted, aes(x=gap.edge.density, y=pred)) +
  geom_line() +
  geom_ribbon(aes(ymin=right.lwr, ymax=right.upr), alpha=0.1) +
  geom_point(data=ba.data, aes(x=gap.edge.density, y=quality.index)) +
  theme_classic()
```

```{r}
summary(ba.top.model)
```

```{r}
# Look at some diagnostics.
data.frame(predicted=predict(ba.top.model, type='response'),
           residuals=residuals(ba.top.model, type='response')) %>% 
  ggplot(aes(x=predicted, y=residuals)) +
  geom_point() +
  geom_hline(yintercept=0, linetype='dashed') +
  geom_smooth(method='lm', se=FALSE, color='black') +
  theme_classic()
```

Again, residuals are worse.

### All models

```{r}
ba.models %>% mutate(glance=map(model, glance)) %>% 
  unnest(glance) %>% ungroup() %>% 
  select(!c(size, data, model))
```

```{r}
ba.models %>% mutate(tidy=map(model, tidy)) %>% 
  unnest(tidy) %>% ungroup() %>% 
  select(!c(size, data, model)) %>% 
  filter(term != '(Intercept)')
```

## Home range

```{r}
aictab(hr.models$model, modnames=hr.models$modname) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```
No point in digging in to the models here, since none of them are any good, but here's some more general info.

### All models

```{r}
hr.models %>% mutate(glance=map(model, glance)) %>% 
  unnest(glance) %>% ungroup() %>% 
  select(!c(size, data, model))
```

```{r}
hr.models %>% mutate(tidy=map(model, tidy)) %>% 
  unnest(tidy) %>% ungroup() %>% 
  select(!c(size, data, model)) %>% 
  filter(term != '(Intercept)')
```

## Maximum range

```{r}
aictab(mr.models$model, modnames=mr.models$modname) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```

### All models

```{r}
mr.models %>% mutate(glance=map(model, glance)) %>% 
  unnest(glance) %>% ungroup() %>% 
  select(!c(size, data, model))
```

```{r}
mr.models %>% mutate(tidy=map(model, tidy)) %>% 
  unnest(tidy) %>% ungroup() %>% 
  select(!c(size, data, model)) %>% 
  filter(term != '(Intercept)')
```

So again, larger scales are meaningless, but VRI-based composition consistently does better than HSI-based composition even when not really significant.




