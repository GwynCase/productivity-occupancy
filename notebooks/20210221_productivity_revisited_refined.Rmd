---
title: "Return to reproductive success - done better"
output: html_notebook
always_allow_html: true
---

```{r options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE)
```

With a larger sample size, how do environmental variables do with reproductive success?

This isn't a very sensible question. It makes more sense to ask whether environmental variables affect diet, but I don't have my full diet dataset yet. Or to ask wether diet affects reproductive success, but I don't have my full diet dataset yet. Plus it looks like my environmental variables are about to become useless if I can't find something helpful to do with them. So.

```{r message=FALSE, warning=FALSE}
# Import conflict settings.
source('../src/conflicted.R')

# Load some libraries.
library(tidyverse)
library(lme4)
library(broom)
library(modelr)
library(AICcmodavg)
library(MuMIn)
library(knitr)
library(kableExtra)

# Load in the habitat variables prepped earlier.
ls <- read_csv('../data/processed/landscape_metrics_full.csv')

# And load in the productivity data.
pd <- read_csv('../data/raw/productivity.csv')

# Join them together, keeping just sites with productivity data.
df <- left_join(pd, ls)

# Look at them.
df %>% distinct(site, quality.index, year, n.fledge)
```

When I first ran this I used `site` as the random effect, which caused convergence issues. After talking it over with the amazing statsbeerz folks, I realized this is probably because, since there is a single quality index value per each site, using `site` as the random effect is the same as using `quality.index` as the random effect--essentially using it as both the explanatory variable *and* the random effect.

Here's a single model as a test:

```{r echo=FALSE, eval=FALSE}
pr.qi <- distinct(df, site, year, n.fledge, quality.index)

pr.b.qi <- lmer(n.fledge ~ quality.index + (1|year), data=pr.qi)

summary(pr.b.qi)
```

Create a bunch of functions for creating a bunch of models. These are (mostly) the same I used for the glms, except I didn't bother to calculat HSI diversity so this is missing `m.suitable.diversity` and `m.suitable.sink`.

```{r}
# Proportion suitable
proportion.suitable.model <- function(df) {
  lmer(n.fledge ~ proportion.suitable + (1|year), data=df)
}

# Proportion suitable + suitable edge density
suitable.edge.density.model <- function(df) {
  lmer(n.fledge ~ proportion.suitable + suitable.edge.density + (1|year), data=df)
}

# Proportion mature forest
proportion.mature.model <- function(df) {
  lmer(n.fledge ~ proportion.cover.mature + (1|year), data=df)
}

# Proportion mature + landcover diversity
mature.diversity.model <- function(df) {
  lmer(n.fledge ~ proportion.cover.mature + cover.diversity + (1|year), data=df)
}

# Proportion mature + gap edge density
mature.edge.density.model <- function(df) {
  lmer(n.fledge ~ proportion.cover.mature + gap.edge.density + (1|year), data=df)
}

# Proportion mature + gap edge density + landcover diversity
mature.sink.model <- function(df) {
  lmer(n.fledge ~ proportion.cover.mature + 
        gap.edge.density + cover.diversity + (1|year), data=df)
}

# Null
null.model <- function(df) {
  lmer(n.fledge ~ (1|year), data=df)
}
```

Then nest the data frame for ease of coding and apply each function to generate a disturbingly large number of models really fast.

```{r warning=FALSE}
# Nest the data frame.
nf <- df %>% group_by(size) %>% nest()

# Apply the functions.
nf <- nf %>% 
  mutate(
    m.proportion.suitable=map(data, proportion.suitable.model),
    m.suitable.edge.density=map(data, suitable.edge.density.model),
    m.proportion.mature=map(data, proportion.mature.model),
    m.mature.diversity=map(data, mature.diversity.model),
    m.mature.edge.density=map(data, mature.edge.density.model),
    m.mature.sink=map(data, mature.sink.model),
    m.null=map(data, null.model)
         )
```

Now error-free!

I'll make it long and add some model quality assessors.

```{r message=FALSE}
# Twist and assess.
nf.long <- nf %>% 
  pivot_longer(-c(size, data), names_to='modname', values_to='model') %>% 
  mutate(glance=map(model, glance)) %>% 
  mutate(rsq=map(model, r.squaredGLMM)) %>% 
  mutate(rsq=map(rsq, as.data.frame))

# Take a look.
nf.long %>% unnest(glance) %>% 
  unnest(rsq) %>% 
  select(-model, -data) %>% 
  arrange(desc(R2m)) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```

Well, they all have terrible R2s. Let's look at the real AICs, since these AICs are moderately useless.

```{r message=FALSE, warning=FALSE}
all.models <- nf %>% pivot_longer(-c(size, data), names_to='modname', values_to='model') %>% 
  mutate(name=paste(size, modname))

aictab(all.models$model, modnames=all.models$name) %>% 
  kable() %>% kable_styling(bootstrap_options=c('striped'))
```

hahahahaha yes the nulls beat everything by a truly astonishing margin. Mind you, this does throw an error so I need to look into that, but it sure looks like these models are pretty useless. But the code can be entirely recycled for `proportion.squirrel` which is more logical and might turn up something different.



